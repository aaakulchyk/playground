{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression Intuition for Dummies.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laneboi/playground/blob/master/Linear_Regression_Intuition_for_Dummies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bd_sWGyEZs",
        "colab_type": "text"
      },
      "source": [
        "# Training of Linear Model\n",
        "\n",
        "As I am just a beginner in Machine Learning and I have no strong intuition in underlying concepts like operations in linear algebra, it is not an easy task to comprehend the meaning of analytical solutions for linear regression. In the Internet different tutorials on linear algebra can be found, and some of them are \"specially designed\" for machine learning/data science entrants. The problem I see there is that even after taking one or more of those (as I did), you may still struggle in applying newly learned concepts on the spot to develop some intuition about what's going on underneath the hood. This notebook is how I explained theoretical equations from the 4th chapter of the textbook by Aureilen Geron (my version is in Russian, so don't worry if my translation mismatches the original text). Please do not consider the text below as something 100% right. If you spot a mistake, please, let me know how to correct it. Hope this notebook will be helpful for somebody else too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVX2rwSt0shY",
        "colab_type": "text"
      },
      "source": [
        "## Closer Look at Linear Regression Model\n",
        "\n",
        "Among known machine learning algorithms, something called linear model regression is one of the simplest (if not the absolutely simplest one). It is easy to express in words how it works: the algorithm is trying to find a straight line (in case of two dimensions, which is the simplest case) such that it fits all the data points the best on average. Let's visualize it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnn2ju2x7OW",
        "colab_type": "code",
        "outputId": "e5e4c8c5-ce6a-46a4-b20b-64753d7099f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "np.random.seed(21) # Initialize seed in order to make the results reproducable.\n",
        "X = 4 * np.random.rand(100, 1)\n",
        "y_ = 5 + 2 * X\n",
        "y = y_ + np.random.randn(100, 1) # Add some random noise.\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4BEkNABIBEkdABIBAkdABLx/2brhlurNPsEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVaX18-ph3wu",
        "colab_type": "text"
      },
      "source": [
        "### Linear Function Written in a Linear Algebra Way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HpPTizh3BfA",
        "colab_type": "text"
      },
      "source": [
        "As you can find in Google, the algorithm uses some technique called \"least squares method\" to find the line with the best fit on average. Long story short, it searches for a line such that the sum: \\begin{equation}\\sum^n_{i=1}(h(x_i)\n",
        "-y_i)^2\\end{equation}\n",
        "is at its minimum. This just means that we calculate the difference between predicted and actual values for each entity, square it, and then sum them all together. Nothing very special here, if we don't go deeper (and we won't do it here)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl2shQT5pJBq",
        "colab_type": "code",
        "outputId": "d6581669-11dd-472d-da8e-952602d3e8fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.plot(X, y, 'b.', label='Dataset')\n",
        "plt.plot(X, y_, 'r-', label='Least Squares Solution')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSugiIimhhC4ikhJK6CIiKfH/ARI4PIrBdCnxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noxoSxDqpi7u",
        "colab_type": "text"
      },
      "source": [
        "Since there is only one such line, there is also an analytical approach (i.e. deriving a formula based on a bunch of theorems) to find the equation for this line. Let's have a look at some equations and try to discover analogies between \"common\" forms and matrix forms.\n",
        "\n",
        "In general, the *hypothesis function* (*h(x)* that makes predictions based on *n* features) can be written as follows:\n",
        "\\begin{equation}h(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n=\\theta_0+\\sum^n_{i=1}{\\theta_ix_i}\\end{equation}\n",
        "or in matrix form:\n",
        "\\begin{equation}h(x)=\\theta^Tx\\end{equation}\n",
        "Hence,\n",
        "\\begin{equation}\\theta^Tx=\\theta_0+\\sum^n_{i=1}{\\theta_ix_i}\\end{equation}\n",
        "***θ*** is a coefficient vector, which is the synonym for list of coefficients. Notice that **x** is also a vector, but one that contains features of a particular entity. For example, it can be a list with age, area, number of rooms and so on of a house. If we use n features in our model, we must have n coefficients to make predictions. Rules for matrix multiplication differ from rules for multiplication of real numbers. To ensure that multiplication is valid, we need to check the shapes of two matrices. Say, an *m × r* matrix can be multiplied only by *r × n* matrices. That's it, to perform matrix multiplication, two inner numbers must be equal! In our case, *θ* and *x* will both be vectors (shape 100 × 1), so we need to transpose *θ*, so that its single column becomes a single row (1 × 100), and we proceed. The dot product of 1 × 100 shape by 100 × 1 shape results in a 1 × 1 solution. This makes sense, because in this example we tried to predict an outcome for only 1 entity with 100 attributes. Hopefully, this equation shines more intuitively clear for you now. Check the code below anyway."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GgwA-GKDi2g",
        "colab_type": "code",
        "outputId": "839fde3f-fcc6-4a2e-c205-fb17642efaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# y = 5 + 2 * x\n",
        "theta = np.array([5, 2])\n",
        "x = np.array([1, 2])\n",
        "print(theta.T.dot(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po6TaS6KgAnu",
        "colab_type": "code",
        "outputId": "c32a6fa6-d825-4492-94f8-a38583fdd418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Adding more features: y = 25 + 4 * x1 - 7 * x2 - 16 * x3 + 2 * x4\n",
        "theta = np.array([25, 4, -7, -16, 2])\n",
        "x = np.array([1, 13, 2, 0, 1.5])\n",
        "print(theta.T.dot(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XzPVsTrgG5W",
        "colab_type": "text"
      },
      "source": [
        "Notice that if the intercept (*θ*[0]) is included in theta as its first value, then *x* must have a 1 on front."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIJkHT0KiOBu",
        "colab_type": "text"
      },
      "source": [
        "Similar job (with additional operations) is done to find losses of the model. The equation for mean squared error in matrix format looks like this:\n",
        "\\begin{equation}MSE(X, h_\\theta)=\\frac{1}{m}\\sum^m_{i=1}(h(x_i)-y_i)^2=\\frac{1}{m}\\sum^m_{i=1}(\\theta^Tx_i-y_i)^2\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIE0sXFrhMy1",
        "colab_type": "text"
      },
      "source": [
        "And we move on to the next equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJWAUAYSjWjO",
        "colab_type": "text"
      },
      "source": [
        "### Black Magic to Find Coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg9m6MaPjdD-",
        "colab_type": "text"
      },
      "source": [
        "This equation actually drove me to reflect on equations in matrix form deeper, because previous didn't look like absolutely unclear and was kinda intuitive. But not this one. Ladies and gentlemen, the crazy ***normal equation***. This is a *closed-form solution* (using a bit of fancy math) for linear regression. We can just paste our data in it, perform some calculations and obtain coefficients that minimize losses. Cool, huh!\n",
        "\\begin{equation}\\hat{\\theta}=(X^TX)^{-1}X^Ty\\end{equation}\n",
        "In order to comprehend this, I decided to look at something similar:\n",
        "\\begin{equation}A=(X^TX)^{-1}X^TX\\end{equation}\n",
        "If you remember the row-column rule for matrix multiplication, we need the inner sizes of two matrices to be equal (m × r and r × n). When we transpose a matrix, its rows and columns are swapped, so `X.T @ X` in terms of sizes can be viewed as `(n × m) @ (m × n) == (n × n)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ockNDAhEZy",
        "colab_type": "code",
        "outputId": "8dea1b7d-11c2-4178-d7f2-69a9d1a0553b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "M = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "M_mult = M.T.dot(M)\n",
        "print(f'{M.T.shape} x {M.shape} = {M_mult.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2) x (2, 3) = (3, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvqamCdzrlCp",
        "colab_type": "text"
      },
      "source": [
        "Now we have a special matrix with equal count of rows and columns - a ***square matrix***. I'll tell you later why we care about this. But for now consider the next two steps. First, we raise the dot product to the power of -1, which is equivalent to division of 1 by this product. In the world of linear algebra, it is called the ***inverse*** of a matrix. Second, we multiply the inverse of dot product by the dot product. There's a parallel to real numbers arithmetic. `1 / n * n == 1`, so `(X.T @ X) ** -1 @ (X.T @ X) == 1`? Not quiet. In case of matrices a multiplication like this one results in something called an ***identity matrix***. Identity matrix is an m × m matrix (i.e. a square matrix) with all 1's on its *main diagonal* (from top left to bottom right) and 0's elsewhere.\n",
        "\\begin{equation}I_3=\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ocKjvhv7M2",
        "colab_type": "text"
      },
      "source": [
        "Back to the normal equation and its little sibling. We can write:\n",
        "\\begin{equation}A=(X^TX)^{-1}X^TX = I_n\\end{equation}\n",
        "\\begin{equation}\\hat{\\theta}=(X^TX)^{-1}X^TX=(X^{-1}A)y=(X^{-1}I_n)y\\end{equation}\n",
        "\n",
        "Which now is the straightforward definition of a coefficient of a linear function. Its interpretation in a real numbers algebra is:\n",
        "\\begin{equation}\\hat{\\theta}=1\\frac{y}{x}=\\frac{y}{x}\\end{equation}\n",
        "\n",
        "At this point you may wonder, why we have to perform all these transpositions and matrices dot multiplications, when we could just multiply the inverse of X by y? Well, we couldn't. And the reason is that not every matrix has the inverse. If it has, such matrix is said to be ***invertible***. Otherwise, a matrix is ***singular*** (*noninvertible*).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC3lFCYFsWBo",
        "colab_type": "code",
        "outputId": "12744c9b-d345-4e6e-e44e-f08f1950052c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    np.linalg.inv(M)\n",
        "except np.linalg.LinAlgError:\n",
        "    print('Whoops!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Whoops!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3XJEFMYtlvx",
        "colab_type": "text"
      },
      "source": [
        "A matrix must satisfy two properties to be invertible:\n",
        "\n",
        "\n",
        "1.   It must be a square matrix\n",
        "2.   Its determinant must be a non-zero value\n",
        "\n",
        "These properties arise from the equation for the inverse of a matrix:\n",
        "\\begin{equation}A^{-1}=(\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix})^{-1}=\\frac{1}{det(A)}\\begin{bmatrix}d & -b \\\\ -c & a\\end{bmatrix}\\end{equation}\n",
        "\n",
        "The determinant of a 2 × 2 matrix is calculated as follows:\n",
        "\\begin{equation}det(A)=det(\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix})=ad-bc\\end{equation}\n",
        "\n",
        "Here I'm not going to explain the meaning of the determinant. If you want more details, I recommend you to watch a series of videos on [linear algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) by [3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/featured) for nice explanations of basic terms of linear algebra and their meanings.\n",
        "\n",
        "In the reality we almost always deal with non-square matrices, and even if they are square, the determinant may still be undefined.\n",
        "\n",
        "To summarize, the process of finding coefficients is really straightforward in its core: we just divide *y* by *x*. Due to restrictions introduced by laws of linear algebra, we have to take a detour to guaranteed perform it. When we write it in matrix form, then the really simple idea is expressed in a fancy way (for newbies in linear algebra). So now I hope it doesn't seem fancy for you, as it seemed to me.\n",
        "\n",
        "One more thing I'd like to discuss here is related to cases with multiple attributes. If we have something like:\n",
        "\\begin{equation}y=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_3\\end{equation}\n",
        "\n",
        "then finding corresponding coefficients is essentially equivalent to finding *partial derivatives*. In other word, how much changes *y* with change of a particular attribute (*x1*, *x2*, *x3*). Thus, in case of linear equation, the vector of coefficients is a vector of partial derivatives:\n",
        "\\begin{equation}\n",
        "\\theta = \n",
        "\\begin{bmatrix}\n",
        "\\theta_1=\\frac{\\partial}{\\partial x_1}y \\\\\n",
        "\\theta_2=\\frac{\\partial}{\\partial x_2}y \\\\\n",
        "\\theta_3=\\frac{\\partial}{\\partial x_3}y \\\\\n",
        "... \\\\\n",
        "\\theta_n=\\frac{\\partial}{\\partial x_n}y\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "It makes sense. Coefficients in a linear equation mean how much we need to add when corresponding variable was increased by one."
      ]
    }
  ]
}